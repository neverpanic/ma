% vim:noet:sts=2:ts=2:sw=2:smarttab:tw=120

\chapter{Evaluation}
	\label{chapter:eval}
	In order to determine whether the optimizations implemented in this thesis actually improve the runtime behavior of
	compiled programs, its results should be compared to systems generated without the optimizations. A series of criteria
	can be evaluated to find differences between different runs. Some of them can be determined without running the
	generated program, such as code size and the number of optimized allocations. Others require measurements at runtime,
	such as heap memory usage or execution speed.

	Any measured application should be as close as possible to real-world usage to yield representative results. On the
	other hand, any benchmark should produce output that can be easily processed, compared an graphed. Rather than using
	a series of micro-benchmarks targeting a certain aspect of the system, previous work on KESO used an open source
	real-time Java benchmark family for embedded systems~\cite{erhardt:11:diplom, erhardt:11:jtres, stilkerich:11:cpe}.
	This benchmark is called CD\textsubscript{x}, its Java variant CD\textsubscript{j}, and was published by Kalibera et
	al.\ in 2009~\cite{kalibera:09:jtres}. It consists of two main components:
	\begin{inparaenum}[(a)]
		\item an \emph{air traffic simulator} that generates a stream of radar frames and passes them to
		\item the \emph{collision detector}, which scans the radar frames for potential aircraft collisions.
	\end{inparaenum}

	The KESO project uses two variants of this benchmark depending on the size restrictions of the target platform. The
	\emph{on-the-go} variant generates the radar frames in the collision detector task and avoids the overhead of passing
	the frames between the two components. At the cost of less realism, this modification significantly shrinks the size
	of the generated binary and reduces the memory requirements. Due to the lower system requirements, this version of the
	benchmark fits and runs on an Infineon TriCore TC1796 board used for testing.

	The second, larger version of the CD\textsubscript{j} benchmark used to test the KESO compiler and its optimization
	result is called the \emph{simulated} variant. This type runs the air traffic simulator in a separate protection
	domain and passes the generated frames to the collision detector using a queue. When frames are generated faster than
	they can be processed (i.e., when deadlines are not met), frames are dropped. Due to heap size requirements and code
	size, this variant of the benchmark does not fit on the TriCore board. Since runtime measurements in a simulated OSEK
	or AUTOSAR OS environment are heavily affected by jitter, time-sensitive measurements are only conducted using the
	\emph{on-the-go} variant.

	The test setups consisting of relevant compiler and software versions and system specifications are given
	in~\cref{tbl:eval:setup}. Measurements of the \emph{on-the-go} benchmark always use the TriCore system, others are
	built and run on Linux using an OSEK emulation layer. These emulation layers are either JOSEK~\cite{josek:10} or
	Trampoline~\cite{bechennec:06:etfa}.

	\begin{table}
		\centering
		\begin{tabular}{rll}
				& \textbf{TriCore system} & \textbf{Linux system}\\\hline\hline
			\multirow{1}{*}{\textbf{CPU}} &
				  Infineon~TriCore~TC1796 & Intel~Core~i5~650, 3.20~GHz\\
				& 150~MHz~CPU, 75~MHz~Bus & \\
			\multirow{1}{*}{\textbf{Memory}} &
				  2~MiB~internal Flash & 7817~MiB~DDR3~PC1333\\
				& 1~MiB~external~SRAM & \\
			\textbf{OS} &
				CiAO~\texttt{4c19874} & Ubuntu~13.10, Linux~3.11\\
			\textbf{Compiler} &
				TriCore~GCC~4.5.2, Binutils~2.20 & GCC~4.8.1, Binutils~2.23.52\\
			\textbf{KESO} &
				\multicolumn{2}{c}{r4072}
		\end{tabular}

		\caption{Hard- and software configurations for the benchmarks}
		\label{tbl:eval:setup}
	\end{table}

	\section{Static Results}
		\label{sec:eval:static}
		The number and percentage share of optimized allocations can be used as a compile time criterion for the quality of
		KESO's optimizations. The higher the number and share of automatically managed objects, the lower the heap load,
		which possibly reduces garbage collector usage. \Cref{fig:eval:static:numallocs} lists the number of stack
		allocations, task-local heap allocations and the total number of allocations in the CD\textsubscript{j}
		\emph{on-the-go} benchmark. For the number of stack allocations without using scope extension, the share of
		optimizations fell from 34.4~\% before this thesis to 30.1~\%. This drop is caused by the removal of 43 allocations
		likely due to improved removal of unused fields, which has been added to KESO between these measurements. Using
		task-local heaps instead of stack allocation increases the percentage of optimized allocation sites to 39.0~\%. The
		13 additional optimizations are local objects with overlapping liveness regions that are left unmodified in stack
		allocation to avoid unbounded growth of stack usage. Enabling scope extension in the same measurement adds another
		28 allocations created by copying allocation bytecode instructions into multiple callers. This will likely also
		increase code size (see also~\cref{sec:eea:probs}). The 28 additional allocations are created instead of 12
		allocation sites that are eligible for scope extension. Each of the dozen allocations is thus propagated into
		3.33\footnote{$\dfrac{174 - (146 - 12)}{12}$} callers on average. The number of stack allocations increases by 32
		from 44 (30.1~\%) to 76 (43.7~\%). Note that these are statically determined numbers, i.e., the actual number of
		objects allocated at runtime does not change despite the increase in allocation instructions. The number of
		allocations not converted into stack allocations due to overlapping liveness regions of the allocated objects stays
		the same. Consequently, the number of allocations using task-local heaps stays at the same margin to stack-allocated
		ones in comparison to the measurement without scope extension.

		\begin{figure}
			\centering
			\input{measurements/2014-06-21-onthego-tricore/numallocs.tex}

			\caption[Number of stack and task-local allocations in \emph{on-the-go} CD\textsubscript{j}]{%
				Number of stack and task-local allocations in the \emph{on-the-go} CD\textsubscript{j} before this thesis and
				after this thesis with and without scope extension.}
			\label{fig:eval:static:numallocs}
		\end{figure}

		Results for the \emph{simulated} variant (given in~\cref{fig:eval:static:numallocs:simulated}) show similar behavior
		albeit with lower percentages of optimized statements. These are caused by the much lower share of \emph{local}
		objects relative to the total amount of allocations. Between the results from before this thesis and those without
		scope extension, the number of total allocations was reduced again, likely due to removal of unused fields. The
		percentage of allocations that use stack memory stayed roughly equal (18.1~\% vs.~18.0~\%). Scope extension
		increases the total number of allocation sites by 74 to 109.3~\%. This should also result in a significant increase
		of the code size. Another 28 allocations are eligible for stack allocation after scope extension. Again, number of
		objects with overlapping liveness regions hardly changes between the measurement with and without scope extension
		â€“ in the \emph{simulated} variant, it increases by 29 (one allocation more than stack allocation) from 20.3~\% to
		21.9~\% of all allocations.

		\begin{figure}
			\centering
			\input{measurements/2014-06-25-simulated-trampoline/numallocs.tex}

			\caption[Number of stack and task-local allocations in \emph{simulated} CD\textsubscript{j}]{%
				Number of stack and task-local allocations in the \emph{simulated} CD\textsubscript{j} before this thesis and
				after this thesis with and without scope extension.}
			\label{fig:eval:static:numallocs:simulated}
		\end{figure}

		As expected, stack allocation and task-local heap allocation increase the size of the code. In the \emph{on-the-go}
		variant shown in~\cref{subfig:eval:static:size:onthego} increases once escape analysis is enabled. This increase is
		caused by inlining the code that initializes an object's header data. Previously, this initialization was only
		present in a single place (the allocation function) in the binary. Because stack allocations have been added in
		multiple places, this initialization code gets replicated and increases the binary size. Additional runtime code
		further increases the code size. New runtime functions and the explicit creation and destruction of regions at entry
		and exit points of methods increase the text segment size when task-local heaps are used. As predicted
		in~\cref{sec:eea:probs} scope extension further increases the size of the code unless methods with candidates for
		the optimization only have a single caller. Since the \emph{on-the-go} variant extends variable scope into 3.33
		callers on average, growth of the text segment is expected. Overall, the text segment's size increases only
		moderately with a maximum of 104.0~\% compared to the smallest selection.

		For the \emph{simulated} CD\textsubscript{j} benchmark in~\cref{subfig:eval:static:size:simulated}, code size
		behaves similar when enabling escape analysis, both with the stack and task-local heap allocation backends. Again,
		task-local heaps need a little more space, but the growth is smalled in comparison with the code size ($\le$ 1.2~\%
		relative to the variant with stack allocation). For the simulated benchmark, enabling scope extension significantly
		increases code size by up to 20.3~KiB or 9.3~\%. Most of the additional allocations are created by a small number of
		methods (e.g., a generated parser) that, however, allocate a large number of escaping objects. Limiting the number
		of optimizations per method as suggested in~\cref{sec:eea:probs} or other heuristic limits could stem this problem.

		\begin{figure}
			\centering
			\subcaptionbox{\emph{On-the-go} variant\label{subfig:eval:static:size:onthego}}[\textwidth]{%
				\centering%
				\input{measurements/2014-06-21-onthego-tricore/codesize.tex}%
			}\\[2ex]
			\subcaptionbox{\emph{Simulated} variant\label{subfig:eval:static:size:simulated}}[\textwidth]{%
				\centering%
				\input{measurements/2014-06-25-simulated-trampoline/codesize.tex}%
			}%
			\caption[CD\textsubscript{j} text segment sizes]{%
				Text segment sizes of the CD\textsubscript{j} benchmark before optimization (plain), after escape analysis with
				stack allocation (EA+stack), after escape analysis with task-local heaps (EA+TLH), after scope extension with
				stack allocation (SE+stack), and after scope extension using task-local heaps (SE+TLH).}%
			\label{fig:eval:static:size}
		\end{figure}

		The data segment size does not change for stack allocation. When using task-local heaps, each configured task-local
		heap adds two additional pointers to the data segment. For the \emph{on-the-go} variant, the size of the data
		section grows by 24 bytes (three task-local heaps). The larger \emph{simulated} variant uses four tasks â€“ its data
		segment size increases by 32 bytes.

	\section{Runtime Results}
		\label{sec:eval:runtime}
		While static analysis shows that a significant share of allocations are optimized, it is not immediately obvious
		that the automatic memory management used reduces the benchmark's runtime. While reducing heap memory usage and the
		amount of memory managed by a garbage collector itself is a worthwhile end, using a garbage collector might still be
		the method of choice if the optimization would add a large runtime overhead. Runtime results will also determine
		whether the optimized allocations are located in heavily used parts of the application, or outside the standard
		control flow paths (e.g., in error handling code).

		\begin{figure}
			\centering
			\input{measurements/2014-06-21-onthego-tricore/relmemory.tex}

			\caption[Relative memory usage of \emph{on-the-go} CD\textsubscript{j}]{%
				Number of stack and task-local allocations in the \emph{simulated} CD\textsubscript{j} before this thesis and
				after this thesis with and without scope extension.}
			\label{fig:eval:static:numallocs:simulated}
		\end{figure}

		\todonote{This is awesome: we are saving up to 50\ \% of memory in CDj at runtime. That's cool! Also, the runtime is
		significantly reduced. Consider measuring GC runtimes and load.}
