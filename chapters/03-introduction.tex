% vim:noet:sts=2:ts=2:sw=2:smarttab:tw=120

\chapter{Introduction}
	\label{chapter:intro}
	Embedded devices are on the rise. While internet-connected refrigerators, which have been journalists' favorite
	prediction for the future, are still a long way off, other turing complete devices have found their way into daily
	life. Taking a look around an ordinary kitchen reveals devices like a fully automatic coffee machine, a digital
	kitchen scale, a dish washer, or even a WiFi-enabled bar code scanner to update shopping lists. All of those contain
	microcontrollers, each serving a special purpose. And it seems these are just the first vistas of an emerging trend:
	At the \emph{embedded world} conference 2014 in Nuremberg, the \emph{Internet of Things}, i.e.\ the interconnection of
	these embedded devices, was a very popular topic. The world might be on the verge of being flooded with highly
	integrated and networked microcontroller-driven devices.

	% See http://www.is2t.com/java-in-embedded-systems/ http://www.eetimes.com/document.asp?doc_id=1318980
	% http://www.eetimes.com/document.asp?doc_id=1317511
	% http://www.eetimes.com/document.asp?doc_id=1262568
	% http://www.eetimes.com/document.asp?doc_id=1261676
	% http://www.eetimes.com/document.asp?doc_id=1319569

	These single-purpose embedded devices are commonly programmed in C and the like. However, with increasing software
	complexity of these devices, Java has been emerging as an appropriate alternative for a number of reasons. Java's ease
	of use, large standard library and suitability for complex projects are increasingly called for even in embedded
	software. Its type safe design prevents memory faults that could be caused by out of bounds array accesses or mistakes
	in manual memory management, eliminating a whole class of potential bugs. Case studies found these improvements to be
	not only theoretical~\cite{phipps:99:spe}.

	These advantages used to come at the price of reduced performance and increased memory usage due to the need for
	runtime environment and garbage collection, but are challenged by new virtual machines and compilers.
	\emph{Ahead-of-time} compilation, i.e.\ translation to native code before execution, has been gaining popularity
	lately: Google revealed switching its Android platform to a new runtime environment, which is expected to
	\enquote{speed up apps by around 100~\%}~\cite{anthony:13:android-art} using ahead-of-time
	compilation~\cite{lindner:14:android-art}. Microsoft declared a similar intention for its .NET
	platform~\cite{lardinois:14:dotnet-aot}. Oracle and others have developed Java virtual machines targeted at embedded
	systems~\cite{merritt:13:java-for-IoT, maxfield:12:IS2T-JVM}.

	\section{The KESO Multi-JVM}
		\label{sec:intro:keso}
		The KESO Java virtual machine uses ahead-of-time compilation and assumes all application code is available for
		analysis at compile time. This \enquote{closed world} assumption makes aggressive optimizations possible. KESO
		systems are built on top of \glspl{rtos} implementing either OSEK/VDX~\cite{OSEKSpec223} or
		AUTOSAR~\cite{autosar:06:sws_os}, specifications used in operating systems for automotive embedded systems. KESO
		allows writing applications and even device drivers in Java and targets deeply embedded systems with real-time
		requirements~\cite{thomm:10:jtres}. KESO's compiler \emph{JINO} analyzes the application code, tailors the runtime
		environment (including the Java standard library) to the application's needs and emits standards-compliant C to be
		compiled for the target architecture.

		\Cref{fig:intro:overview} gives an architectural overview of a KESO system. The runtime environment provides an
		abstraction layer on top of the OSEK/VDX or AUTOSAR \gls{rtos} and allows configurable access to specific memory
		addresses for memory-mapped I/O, e.g.\ in device drivers. KESO also provides a mechanism similar to the \gls{jni} to
		execute native code. A KESO system can contain multiple protection realms (so-called \emph{domains}), each of which
		can have a number of tasks, resources, alarms, and \glspl{isr}, its own heap region, and garbage collection
		mechanism. Domains communicate using a \gls{rpc} mechanism called \emph{portals}. The KESO runtime environment
		ensures objects passed through portals cannot interfere with other protection domains.

		\begin{figure}
			\begin{center}
				\input{images/keso-schematic.tex}
			\end{center}
			\caption[Schematic overview of a KESO system]{%
				Schematic overview of a KESO system. An OSEK or AUTOSAR \gls{rtos} runs on a microcontroller. On top of the
				operating system, the KESO runtime environment provides services and abstractions used by the application, such
				as \gls{rpc} primitives or device drivers. Multiple protection realms (\emph{domains}) can contain multiple
				tasks each, have their own resources, heap, and garbage collector and communicate safely using \emph{portals}.}
			\label{fig:intro:overview}
		\end{figure}

		Because Java is a type safe language, KESO can employ a combination of compile-time and runtime checks to ensure
		that applications cannot modify memory outside their protection realm even in the absence of specialized hardware
		for this task, such as a \gls{mpu} or \gls{mmu}. Since KESO guarantees complete isolation even when one of the tasks
		misbehaves, multiple applications can be run on the same system without interfering with each other, possibly
		further reducing required chip size, energy consumption and production costs. Due to the reduction of structure
		sizes in modern computing chips, dealing with transient soft errors such as bit flips is mandatory for critical
		applications. Software-based mechanisms for isolation are at a disadvantage compared to \glspl{mcu} with
		hardware-based memory protection such as \glspl{mpu} and \glspl{mmu}, which offer protection against errors caused
		by this problem class. Previous work on KESO attempts to compensate this~\cite{thomm:11:jtres, stilkerich:13:lctes}.

	\section{Motivation}
		\label{sec:intro:motivation}

		\emph{Manual memory management} using library functions has been the de-facto standard method of dealing with
		dynamic memory needs in C and \C++{}. It provides fine-grained control over applications' memory allocation
		behavior, but comes with a downside: Programming mistakes can lead to leaks and dangling pointers, which in turn can
		lead to security vulnerabilities or crashes. As a consequence, developers need to be careful while writing code that
		uses manual memory management, in particular when used in safety-critical components.

		In order to address these drawbacks, automatic memory management techniques, such as \emph{garbage collection}, can
		be used. Instead of having the software developer deal with unused memory manually, slices of memory that are no
		longer referenced from the working data set are automatically identified and reclaimed, avoiding memory leaks and
		dangling pointers entirely. On the downside, unused memory is not reclaimed until the next garbage collection cycle,
		potentially reducing the predictability of an application's memory usage. Finding tight upper bounds for both
		runtime – the so-called \gls{wcet} – and memory usage is required to determine whether real-time constraints can be
		met. Compared to manual memory management, garbage collection is less error-prone at the cost of not reclaiming
		memory immediately, being less predictable and requiring additional computation. Both manual memory management and
		garbage collection need to deal with fragmentation caused by reclaiming in a sequence that differs from the
		allocation order (\emph{external fragmentation}), further increasing the complexity of memory allocation and
		reclaiming.

		Another alternative that exists in both manual and automatic variants are \emph{region-based} approaches to managing
		memory. Each slice of memory is allocated in one of many memory regions (also called \emph{pools}). Unused regions
		are reclaimed on the whole, avoiding external fragmentation completely. A pool will only be recycled if all of the
		objects allocated in its memory area are no longer referenced. A bad mapping from allocation to memory pool may thus
		prevent the recycling of unused memory; in the worst case a single object can hold a whole pool \enquote{hostage},
		preventing its re-use. On the other hand, region-based memory management does not suffer from external fragmentation
		because it does not reclaim slices from its pools. The time needed for allocation operations is thus easily
		predictable and allocation can be implemented in $\Theta(1)$. This is obvious when considering how allocation
		requests are handled and where the used memory resides: All previously allocated memory is still considered to be in
		use and occupies a single continuous block at the beginning of the memory pool. New requests can be served by
		extending this block, which takes a constant number of operations (moving a level marker). Constant runtime is an
		advantage over both manual memory management and garbage collection, which do not always guarantee tight upper
		bounds for memory management operations\footnote{This is not to say that they cannot, and there are a few algorithms
		that achieve good upper bounds for these operations by avoiding fragmentation or embracing it. See~\cite{strotz:14}
		for previous work in KESO about this problem and~\cite{pizlo:10:pldi} for the work it is based on.}.

		\emph{Manual region-based} methods require developers to map allocation operations to the regions that shall be used
		to satisfy the requests. The Real-Time Specification for Java supports manual region-based approaches to memory
		management with the subclasses of its \texttt{javax.realtime.ScopedMemory} class~\cite{rtsj:06:scoped}. Manual
		methods allow fine-grained control over the application's behavior, but suffer from the same potential problems
		present in (non-region-based) manual memory management, such as dangling pointers.

		\emph{Automatic region-based} approaches infer regions and region assignments at compile time. Previously published
		techniques include semi-automatic methods, combinations of region inference and garbage collection to fully
		automatic region inference~\cite{grossman:02:pldi, hallenberg:02:sigplan, chin:04:pldi}. Most of the work related to
		region inference is based on the work of Tofte and Talpin in 1994 originally aimed at functional
		languages~\cite{tofte:94:popl}. Downsides of region inference in literature include its high algorithmic and
		computational complexity, its suboptimal results for larger programs and the assumption that object lifetimes follow
		a stack discipline. Small source code adjustments are sometimes necessary to achieve good memory
		performance.~\cite{henglein:01:ppdp}

		A special case of manual region-based memory management is \emph{stack allocation}. Each function call creates a new
		logical memory pool that can be used to allocate structures and objects on the stack. The pool is automatically
		reclaimed at the end of the method, so only objects whose lifetime is bounded by the runtime of their allocating
		function can be stored in stack memory. Since stack memory is automatically reclaimed, memory leaks cannot occur.
		Dangling pointers, however, are still possible, for example by returning a pointer to local stack memory. In type
		safe source languages such as Java, compilers can accurately compute which references point to the same memory
		locations (\emph{alias analysis}) and whether any of these references remains live after the method that initially
		defined them terminates (\emph{escape analysis}). Using escape analysis, dangling pointers can be detected at
		compile time and objects which will not escape their method of allocation can safely be allocated in stack memory.
		In the context of real-time systems, automatic stack allocation can reduce the working set of the garbage collector
		and increase the system's predictability.

		Stack allocation using escape analysis has been implemented in my bachelor's thesis~\cite{lang:12}. The scope of
		this thesis is improving the existing analysis (covered in~\cref{chapter:ea}) and extending it to allow more objects
		to be managed automatically without garbage collection (in~\cref{chapter:eea}). To achieve this, an analysis pass
		identifying objects which will outlive their method of allocation but not the calling method, was developed. The
		results of this analysis were used in a transformation that extends the scope of such objects into the calling
		method, enabling their allocation in the caller's stack frame, similar to a pattern often encountered in C programs.
		This transformation can be beneficial in some Java APIs commonly used in generic software, e.g.\ when dealing with
		strings and \texttt{StringBuilder}s. While embedded systems in a car environment typically do not deal with strings
		a lot, the optimization might still be useful when implementing network communication protocols. Although the
		\emph{Internet of Things} is not one of KESO's target domains, the work done in this thesis should improve the
		memory allocation behavior of applications that deal with string-based protocols such as HTTP.

	\section{Previous Work}
		\label{sec:intro:prev}
		Based on the work of Choi et al.\ published in 2003~\cite{choi:03:toplas}, I implemented alias analysis and escape
		analysis in my bachelor's thesis. Different from the paper it was based on, KESO's implementation does not
		dynamically allocate memory on the stack. Instead, allocation sites whose objects' liveness regions overlap are
		allocated in garbage-collected heap memory, keeping the stack usage bounded in the absence of recursion. Since KESO
		targets embedded systems with real-time constraints, predictable stack usage is important.

		Note that most of this work cannot be applied to languages with pointers easily. This is due to the fact that
		pointer arithmetic might not be predictable at compile time, increasing alias analysis complexity. Because
		conservative assumptions must often be made for code using pointers, determining whether an object does not escape
		its method of allocation is much harder~\cite{horwitz:97:toplas, hind:99:toplas, landi:92:loplas,
		ramalingam:94:toplas}.

	\section{Document Structure}
		\label{sec:intro:document-structure}
		\Cref{chapter:ea} describes the changes implemented in KESO's escape analysis for this thesis. It starts with
		a basic summary of the algorithms and data structures used and contains detailed descriptions of improvements and
		the fix for a conceptual flaw in the work of Choi et al. The following~\cref{chapter:eea} outlines the idea of
		improving stack allocation using variable scope extension and explains how this was implemented and which challenges
		I encountered while doing that. The Evaluation in~\cref{chapter:eval} compares the results of my work to previous
		versions of KESO using static results – like the number of automatically managed allocations – and runtime results
		of a number of benchmarks. \Cref{chapter:rel} gives an overview of related work and explains the differences to this
		thesis before~\cref{chapter:conclusion} concludes, gives an overview over the work done for this thesis and
		discusses some ideas for future work.
