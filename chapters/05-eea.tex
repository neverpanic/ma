% vim:noet:sts=2:ts=2:sw=2:smarttab:tw=120

\chapter{Extended Escape Analysis}
	\label{chapter:eea}
	A standard pattern found in C programs is passing a buffer and its size to a function which will write a computed
	result into the given buffer. Since the calling function controls the location of the buffer, it can be allocated from
	stack memory. In Java, a method would instead allocate a new object from heap and return a reference to it to achieve
	the same. Using alias information and escape analysis, objects that escape their method of alloction into the caller
	but no further can be automatically identified. Allocating these objects on the caller's stack and passing a reference
	avoids the need for garbage collection for these objects and can improve worst and average case execution times of
	programs. In the KESO context, this is called \emph{extended escape analysis} or \emph{(variable) scope extension}.

	Stack allocation is not the only possible optimization using these results, and may not be the best. Several other
	approaches such as thread-local heap regions or explicit deallocation operations come to mind. Unbounded usage of the
	optimizations discussed in the chapter may lead to problems and sub-par performance.

	\Cref{sec:eea:idea}~outlines the general idea of the optimization implemented for this master's thesis and gives and
	example showing where, why, and how it can be applied. The following~\cref{sec:eea:analysis} discusses in detail which
	preconditions must be fulfilled, which constellations hinder or prevent optimization, and how these shortcomings could
	be addressed. Two different transformations using the analysis results and their advantages and drawbacks are
	presented in~\cref{sec:eea:opt}, before~\cref{sec:eea:probs} concludes this chapter with a consideration of possible
	problems caused by excessive usage of extended escape analysis.

	\section{Algorithmic Idea}
		\label{sec:eea:idea}
		\inputthesiscode{java}{lst:eea:idea:builder}{Example containing a candidate for extended escape analysis}{%
			Example simplified from the CD\textsubscript{j} benchmark from the CD\textsubscript{x} family of
			benchmarks~\cite{kalibera:09:jtres}. The object allocated in \emph{Factory.getBuilder} does not escape
			\emph{Simulation.run}. It can be allocated on the stack of \emph{Simulation.run}.
		}{examples/Factory.java}
		\Cref{lst:eea:idea:builder} shows an example adapted from the source code of the CD\textsubscript{j}
		benchmark~\cite{kalibera:09:jtres} where the technique can be applied. The \emph{Builder} object allocated in
		\emph{Factory.getBuilder} escapes its allocating method into \emph{Simulation.run}, but is no longer referenced
		after line~\ref{line:eea:idea:builder:lastb}. It can be allocated in the stack frame of \emph{Simulation.run} and
		automatically reclaimed after \emph{Simulation.run} returns. A reference to the object would be passed to
		\emph{Factory.getBuilder} using a new, artificial parameter. The allocation operation in the callee would then be
		replaced with a parameter read. The invocation of the constructor of \emph{Builder} stays in
		\emph{Factory.getBuilder}.

		If the application can be interrupted between stack allocation in the caller and constructor call in the callee
		(e.g., by stop-the-world or on-demand garbage collection or a blocking method call) the referenced memory area must
		be in a defined state. Passing a reference to uninitialized memory (like in C) is not possible unless special
		precautions such as pointer tagging are used.

		Returning to the running example, the \emph{Builder} object can only be reclaimed by garbage collection without
		optimization. With optimization, the stack memory can be reused in later iterations of the loop for the same
		allocation, reducing the number of objects that need garbage collection at runtime.

		The examples discussed so far all deal with objects escaping their method of creation via a return operation. Note
		that being returned is not the only way for an object to escape: storing references in a field of an object given as
		parameter will also increase the escape state. This case is omitted in all examples for simplicity, but always
		implied.

	\section{Analysis}
		\label{sec:eea:analysis}
		Any object in the \emph{method} escape state partition of a method's \gls{cg} is a candidate for optimization. The
		escape state of the object's representation in the method's callers can be taken into account to decide whether the
		object should be allocated in the caller. Note that since there might be multiple callers and the optimization could
		be applied multiple times (moving allocations up multiple levels in the call hierarchy) considering the escape state
		of the object in the callers' \glspl{cg} is not always a trivial task. For example, the object might escape further
		in some of the callers but not in others. Virtual method invocations need to be handled with special care to avoid
		breaking the signature of these methods: all candidates for a virtual method invocation need to share the same
		signature before and after optimizing. See~\cref{sub:eea:analysis:virtual} for a detailed discussion of virtual
		method invocations in the context of extended escape analysis.

		KESO's implementation does not take the escape state of an object node's equivalents in the callers' \glspl{cg} into
		account. For each run of the analysis and optimization pass, allocations are propagated at most a single level up in
		the call hierarchy. Therefore, running the pass multiple times will increase the maximum scope extension level. Note
		that is is not necessarily beneficial to run the pass often, since it may lead to undesirable results (see
		below).\todo{add reference}

		\todonote{Give an overview of the algorithm JINO uses to extend the scope of a variable. Explain how and why it
		works and why we use stack allocation (and make sure to point out that stack allocation isn't the only possible
		optimization and may in fact be not the best). Quote my future work list on what other cool stuff could be done here
		and also outline which problems make this a hard problem.}

		\subsection{Nonvirtual Calls}
			\label{sub:eea:analysis:nonvirtual}
			Nonvirtual call sites, i.e., those where the invoked method is unique and known at compile time, constitute the
			simple cases of the analysis. The KESO compiler tries to increase the number of non-ambiguous invocations by
			devirtualizing method invocations where a single candidate can be deduced using static
			analysis~\cite[Sec.~3.4]{erhardt:11:jtres}.

			Each object node with a known allocation site (i.e., each non-phantom object node) and an escape state of
			\emph{method} will be optimized in KESO\@. Interference information for the allocated objects is not computed.
			This causes multiple allocations to be moved into calling methods even if they are allocated in mutually exclusive
			control flow paths. In some examples, this causes a large number of allocations and new method parameters even
			though only a few are used simultaneously. See~\cref{sec:conclusion:future-work} for possible ways to avoid this
			problem and a discussion of the challenges in solving it.

		\subsection{Virtual Calls}
			\label{sub:eea:analysis:virtual}
			Virtual method invocations further complicate the decision whether to move an allocation into calling methods or
			not. Because all candidates of a virtual method invocation must share the same signature (i.e., the same parameter
			and return types), a method cannot be optimized individually without considering other invocations possibly
			calling this method and these call site's possible callees. \Cref{fig:eea:analysis:virtual} contains a graphical
			representation of this problem. Interdependencies between methods cause methods to form up into groups sharing the
			same signature. Extended escape analysis, however, depends only on the code of the methods in these groups, which
			is in general unrelated. A single method in such a group could cause the optimization to add a number of arguments
			to all its invocations, which in turn would require that the same parameters be added to all other candidates for
			these invocation sites. These parameters would be unused in all other methods and cause overhead at runtime as
			well as the allocation of unused memory.

			\begin{figure}
				\centering%
				\input{examples/vcall-eea.tex}%

				\caption[Call graph showing the complexity of extended escape allocation for virtual method calls]{%
					Call graph showing the complexity of extended escape allocation for virtual method calls. Green
					{\color{cggreen}\blacksquare} vertices mark methods that contain allocations eligible for scope extensions,
					blue {\color{cgblue}\blacksquare} vertices represent other methods. Solid lines are method invocations. Assume
					that both \emph{a} and \emph{b} contain a single virtual method invocation each, i.e., the possible callees
					are \emph{1}–\emph{3} for \emph{a} and \emph{3}–\emph{5} for \emph{b}. Dashed lines point from methods
					eligible for extended escape analysis to methods that must share their signature. Since this relation is
					transitive, nodes \emph{1} through \emph{5} and their invocation sites must be adjusted for each optimization
					in \emph{2}, \emph{3}, and \emph{4}.}%
				\label{fig:eea:analysis:virtual}%
			\end{figure}

			Because of the overhead and the complexity inherent to applying this optimization correctly in the presence of
			virtual method calls, KESO does not currently perform scope extension across virtual method calls. Note that some
			of the challenges are caused by the way the results of alias and escape analysis are used to optimize allocations.
			Different intermediate code transformations that do not require changing a method's signature could simplify the
			problem.

			For example, instead of using stack memory, a separate thread-local heap section with a simple bump pointer memory
			management strategy could be used (see~\cref{sub:eea:opt:ldh} where this is discussed). Memory could be allocated
			in the section corresponding to a calling method in these thread-local heaps during a method's prologue before
			creating a new method frame. Memory allocated in this way would automatically be reclaimed after the calling
			method terminates.

			A different approach to solve the same problem could be not changing the allocations at all (i.e., allocating
			objects that escape only a single level in the call graph in heap memory) and modifying the caller to explicitly
			reclaim the objects that are no longer used. However, in the presence of a garbage collector this does not
			necessarily reduce the memory management overhead: Marking a section as free does neither reduce the time required
			for the mark phase (the unreferenced section will not be marked, whether it was explicitly freed or not), nor the
			sweep phase (the task is the same, it will either by run by the garbage collector, or by the explicit statement).
			As a consequence, the only possible improvement achieved by explicit deallocations could be a reduction in the
			number of garbage collector runs. By keeping a list of memory areas that can be re-used immediately, the runtime
			system could avoid a garbage collector run entirely if enough memory can be made available using the known unused
			areas.

			Because KESO's current escape analysis summarizes a method's effect independent of any calling contexts, but both
			approaches outlined in the last paragraphs depend on the caller, further analyses would have to be implemented to
			use these ideas.

	\section{Optimization}
		\label{sec:eea:opt}
		Based on the results of alias and escape analysis and the decisions presented in~\cref{sec:eea:analysis}, KESO's
		compiler can apply optimizing transformations. The first transformation operates on the intermediate code
		representation of the compiled program and moves certain allocations into a method's callers. To preserve soundness
		a reference to the allocated memory is instead passed as argument to the method previously containing the
		allocation. The newly created allocation can potentially be served from stack memory, but~\cref{sub:eea:opt:ldh}
		introduces a different concept to avoid potential problems with excessive stack usage.

		\subsection{Extending Variable Scope}
			\label{sub:eea:opt:scopeext}

		\subsection{Local Domain Heaps}
			\label{sub:eea:opt:ldh}

	\section{Potential Problems}
		\label{sec:eea:probs}
		\todonote{What problems and undesirable results could occur due to excessive usage of the optimization?}
